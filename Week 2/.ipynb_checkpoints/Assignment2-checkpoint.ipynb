{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment2\n",
    "\n",
    "## Notebook made by  \n",
    "\n",
    "|** Name** | **Student id** | **email**|\n",
    "|:- |:-|:-|\n",
    "|. | | |\n",
    "|  | |. |\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. \n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>\n",
    "\n",
    "### Note\n",
    "* **Assignments without the selfies or completely filled in information will not be graded and receive 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: obtaining information from the web\n",
    "\n",
    "### RSS parsing\n",
    "\n",
    "Make a notebook that performs the following steps.\n",
    "\n",
    "1. Create a script that retrieves all urls of rss feeds from <http://www.volkskrant.nl/rss/feeds/>. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
    "    * **update 2016**\n",
    "    * As all Dutch sites, Volkskrant asks whether you accept cookies. This makes simple collecting webpages a lot harder. \n",
    "    * The code in the code cell below does the trick. \n",
    "    * After running this, I could collect further files from Volkskrant without additional cookie hassle.\n",
    "2. Download all rss feeds and store them on disk.\n",
    "3. Parse all RSS feeds using `lxml`. Create a list of  dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
    "4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
    "5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
    "6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
    "7. Count per day the number of words, and the number of unique words. Show this in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cookielib # Thanks to http://stackoverflow.com/questions/29395407/enabling-cookies-with-urllib\n",
    "import urllib2\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "\n",
    "url = 'http://www.volkskrant.nl/rss/feeds/'\n",
    "\n",
    "# with urllib2 and handling cookies\n",
    "cookiejar= cookielib.LWPCookieJar()\n",
    "opener= urllib2.build_opener( urllib2.HTTPCookieProcessor(cookiejar) )\n",
    "response=opener.open(url)\n",
    "html_doc= ' '.join(response.readlines())\n",
    " \n",
    "rsssoup = BeautifulSoup(html_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.volkskrant.nl/nieuws/rss.xml', 'http://www.volkskrant.nl/nieuws-voorpagina/rss.xml', 'http://www.volkskrant.nl/buitenland/rss.xml', 'http://www.volkskrant.nl/binnenland/rss.xml', 'http://www.volkskrant.nl/opinie/rss.xml', 'http://www.volkskrant.nl/economie/rss.xml', 'http://www.volkskrant.nl/sport/rss.xml', 'http://www.volkskrant.nl/tech/rss.xml', 'http://www.volkskrant.nl/media/rss.xml', 'http://www.volkskrant.nl/wetenschap/rss.xml', 'http://www.volkskrant.nl/politiek/rss.xml', 'http://www.volkskrant.nl/cartoons/rss.xml', 'http://www.volkskrant.nl/foto/rss.xml', 'http://www.volkskrant.nl/vonk/rss.xml', 'http://www.volkskrant.nl/authors/rss.xml', 'http://www.volkskrant.nl/toplists/rss.xml', 'http://www.volkskrant.nl/archief/rss.xml', 'http://www.volkskrant.nl/cultuur-en-leven/rss.xml', 'http://www.volkskrant.nl/cultuur-en-leven-voorpagina/rss.xml', 'http://www.volkskrant.nl/recensies/rss.xml', 'http://www.volkskrant.nl/film/rss.xml', 'http://www.volkskrant.nl/muziek/rss.xml', 'http://www.volkskrant.nl/reizen/rss.xml', 'http://www.volkskrant.nl/koken-en-eten/rss.xml', 'http://www.volkskrant.nl/theater/rss.xml', 'http://www.volkskrant.nl/boeken/rss.xml', 'http://www.volkskrant.nl/magazine/rss.xml', 'http://www.volkskrant.nl/beeldende-kunst/rss.xml', 'http://www.volkskrant.nl/mode-en-mooi/rss.xml', 'http://www.volkskrant.nl/festivals/rss.xml', 'http://www.volkskrant.nl/televisie/rss.xml', 'http://www.volkskrant.nl/voordeel/rss.xml']\n"
     ]
    }
   ],
   "source": [
    "#Rss parsing:\n",
    "#Question 1:\n",
    "#rss list\n",
    "rss_list = []\n",
    "list_items=rsssoup.findAll('li')\n",
    "len(list_items)\n",
    "count = 0\n",
    "for x in list_items:\n",
    "    y = str(list_items[count])\n",
    "    if \"rss.xml\" in y:\n",
    "        url = re.search('\"(.+?)\"', y).group(1)\n",
    "        rss_list.append(url)\n",
    "    count += 1\n",
    "\n",
    "print rss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 2:\n",
    "for url in rss_list:\n",
    "    response = opener.open(url)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    title = soup.title.string[3:]\n",
    "    with open (title,'w') as fileOutput:\n",
    "        fileOutput.write(html + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Question 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JSON parsing\n",
    "\n",
    "1. Download <http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb>, remove all code blocks, and turn it into a notebook again. \n",
    "2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
    "3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PDF parsing\n",
    "\n",
    "1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use _difficult_ PDF. Why not try <http://wch.github.io/latexsheet/latexsheet.pdf>?\n",
    "\n",
    "* Is the word order still as it should be?\n",
    "* What about the strange characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2 Python recap\n",
    "\n",
    "Download [PythonRecap2.0.ipynb](PythonRecap2.0.ipynb),  and answer all questions as asked."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
